# -*- coding: utf-8 -*-
"""Crop Yield Predictor .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rl7Ntc_TFoDETdBFogLCeT7INMIWCj6a
"""

# Install all required packages

print("âœ… All libraries installed successfully!")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
import shap
import pickle
import warnings
warnings.filterwarnings('ignore')

# --- Step 1: Download and Load Dataset ---
print("ðŸ“¥ Downloading dataset...")
# !wget -q https://raw.githubusercontent.com/atharvaingle/Crop-Recommendation-Dataset/master/Crop_recommendation.csv
df = pd.read_csv('/content/Crop_recommendation (1).csv')
print(f"âœ… Dataset loaded: {len(df)} rows")

# --- Step 2: Create Synthetic Yield Data ---
print("ðŸ”„ Generating yield data...")

def calculate_yield(row):
    """Calculate crop yield based on soil and environmental factors"""
    base_yield = 20

    # Factor calculations (how each parameter affects yield)
    n_factor = 1 + (row['N'] - 50) * 0.01 if row['N'] >= 50 else 1 - (50 - row['N']) * 0.015
    p_factor = 1 + (row['P'] - 30) * 0.008 if row['P'] >= 30 else 1 - (30 - row['P']) * 0.01
    k_factor = 1 + (row['K'] - 40) * 0.005 if row['K'] >= 40 else 1 - (40 - row['K']) * 0.008
    temp_factor = 1 - abs(row['temperature'] - 25) * 0.02
    rain_factor = 1 + (row['rainfall'] - 600) * 0.0005 if row['rainfall'] >= 600 else 1 - (600 - row['rainfall']) * 0.001
    ph_factor = 1 - abs(row['ph'] - 6.5) * 0.1

    # Calculate final yield
    yield_value = base_yield * n_factor * p_factor * k_factor * temp_factor * rain_factor * ph_factor
    noise = np.random.normal(0, 2)
    return max(10, yield_value + noise)

df['yield_quintals_per_hectare'] = df.apply(calculate_yield, axis=1)
print("âœ… Yield data created")

# --- Step 3: Prepare Data for Training ---
feature_columns = ['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall']
X = df[feature_columns]
y = df['yield_quintals_per_hectare']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"âœ… Data split: {len(X_train)} training, {len(X_test)} testing")

# --- Step 4: Train Two Models ---
print("ðŸ¤– Training AI models...")

xgb_model = XGBRegressor(n_estimators=100, max_depth=8, learning_rate=0.1, random_state=42)
xgb_model.fit(X_train, y_train)

rf_model = RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42)
rf_model.fit(X_train, y_train)

# --- Step 5: Comprehensive Model Evaluation ---
print("\n" + "="*70)
print("ðŸ“Š MODEL EVALUATION & ACCURACY METRICS")
print("="*70)

def evaluate_model(model, model_name, X_train, X_test, y_train, y_test):
    """Comprehensive model evaluation"""

    # Predictions
    train_pred = model.predict(X_train)
    test_pred = model.predict(X_test)

    # Calculate metrics
    train_mae = mean_absolute_error(y_train, train_pred)
    test_mae = mean_absolute_error(y_test, test_pred)

    train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))
    test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))

    train_r2 = r2_score(y_train, train_pred)
    test_r2 = r2_score(y_test, test_pred)

    # MAPE (Mean Absolute Percentage Error)
    train_mape = np.mean(np.abs((y_train - train_pred) / y_train)) * 100
    test_mape = np.mean(np.abs((y_test - test_pred) / y_test)) * 100

    # Accuracy (for regression: 100 - MAPE)
    train_accuracy = 100 - train_mape
    test_accuracy = 100 - test_mape

    return {
        'model_name': model_name,
        'train_mae': train_mae,
        'test_mae': test_mae,
        'train_rmse': train_rmse,
        'test_rmse': test_rmse,
        'train_r2': train_r2,
        'test_r2': test_r2,
        'train_mape': train_mape,
        'test_mape': test_mape,
        'train_accuracy': train_accuracy,
        'test_accuracy': test_accuracy,
        'predictions': test_pred
    }

# Evaluate both models
xgb_metrics = evaluate_model(xgb_model, "XGBoost", X_train, X_test, y_train, y_test)
rf_metrics = evaluate_model(rf_model, "Random Forest", X_train, X_test, y_train, y_test)

# --- Display Results ---
def print_metrics(metrics):
    """Print formatted metrics"""
    print(f"\nðŸ”¹ {metrics['model_name']} Model:")
    print(f"   {'Metric':<25} {'Training':<15} {'Testing'}")
    print(f"   {'-'*55}")
    print(f"   {'MAE (Mean Abs Error)':<25} {metrics['train_mae']:<15.3f} {metrics['test_mae']:.3f}")
    print(f"   {'RMSE (Root Mean Sq)':<25} {metrics['train_rmse']:<15.3f} {metrics['test_rmse']:.3f}")
    print(f"   {'RÂ² Score':<25} {metrics['train_r2']:<15.3f} {metrics['test_r2']:.3f}")
    print(f"   {'MAPE (%)':<25} {metrics['train_mape']:<15.2f} {metrics['test_mape']:.2f}")
    print(f"   {'âœ… ACCURACY (%)':<25} {metrics['train_accuracy']:<15.2f} {metrics['test_accuracy']:.2f}")

print_metrics(xgb_metrics)
print_metrics(rf_metrics)

# --- Choose Best Model ---
best_model = xgb_model if xgb_metrics['test_mae'] < rf_metrics['test_mae'] else rf_model
best_metrics = xgb_metrics if xgb_metrics['test_mae'] < rf_metrics['test_mae'] else rf_metrics

print("\n" + "="*70)
print(f"ðŸ† BEST MODEL: {best_metrics['model_name']}")
print("="*70)
print(f"   Test Accuracy: {best_metrics['test_accuracy']:.2f}%")
print(f"   Test RÂ² Score: {best_metrics['test_r2']:.3f}")
print(f"   Test MAE: {best_metrics['test_mae']:.3f} quintals/hectare")
print("="*70)

# --- Step 6: Visualizations ---
print("\nðŸ“ˆ Creating visualizations...")

# Create a 2x2 subplot
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
fig.suptitle('Model Evaluation Dashboard', fontsize=16, fontweight='bold')

# 1. Actual vs Predicted (Best Model)
ax1 = axes[0, 0]
ax1.scatter(y_test, best_metrics['predictions'], alpha=0.6, edgecolors='k', linewidth=0.5)
ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect Prediction')
ax1.set_xlabel('Actual Yield (quintals/hectare)', fontsize=11)
ax1.set_ylabel('Predicted Yield (quintals/hectare)', fontsize=11)
ax1.set_title(f'{best_metrics["model_name"]}: Actual vs Predicted', fontsize=12, fontweight='bold')
ax1.legend()
ax1.grid(True, alpha=0.3)

# 2. Residuals Plot
ax2 = axes[0, 1]
residuals = y_test - best_metrics['predictions']
ax2.scatter(best_metrics['predictions'], residuals, alpha=0.6, edgecolors='k', linewidth=0.5)
ax2.axhline(y=0, color='r', linestyle='--', lw=2)
ax2.set_xlabel('Predicted Yield (quintals/hectare)', fontsize=11)
ax2.set_ylabel('Residuals', fontsize=11)
ax2.set_title('Residual Plot', fontsize=12, fontweight='bold')
ax2.grid(True, alpha=0.3)

# 3. Model Comparison - Metrics
ax3 = axes[1, 0]
metrics_comparison = pd.DataFrame({
    'XGBoost': [xgb_metrics['test_accuracy'], xgb_metrics['test_r2']*100, 100-xgb_metrics['test_mape']],
    'Random Forest': [rf_metrics['test_accuracy'], rf_metrics['test_r2']*100, 100-rf_metrics['test_mape']]
}, index=['Accuracy (%)', 'RÂ² Score (Ã—100)', 'Inverse MAPE'])

metrics_comparison.plot(kind='bar', ax=ax3, color=['#1f77b4', '#ff7f0e'], width=0.7)
ax3.set_title('Model Comparison - Performance Metrics', fontsize=12, fontweight='bold')
ax3.set_ylabel('Score', fontsize=11)
ax3.set_xticklabels(ax3.get_xticklabels(), rotation=45, ha='right')
ax3.legend(loc='lower right')
ax3.grid(True, alpha=0.3, axis='y')

# 4. Error Distribution
ax4 = axes[1, 1]
ax4.hist(residuals, bins=30, edgecolor='black', alpha=0.7, color='skyblue')
ax4.axvline(x=0, color='r', linestyle='--', lw=2, label='Zero Error')
ax4.set_xlabel('Prediction Error (quintals/hectare)', fontsize=11)
ax4.set_ylabel('Frequency', fontsize=11)
ax4.set_title('Error Distribution', fontsize=12, fontweight='bold')
ax4.legend()
ax4.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

# --- Feature Importance ---
print("\nðŸ“Š Feature Importance Analysis...")
if best_metrics['model_name'] == "XGBoost":
    feature_importance = best_model.feature_importances_
else:
    feature_importance = best_model.feature_importances_

importance_df = pd.DataFrame({
    'Feature': feature_columns,
    'Importance': feature_importance
}).sort_values('Importance', ascending=False)

plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Importance'], color='steelblue')
plt.xlabel('Importance Score', fontsize=12)
plt.ylabel('Features', fontsize=12)
plt.title(f'{best_metrics["model_name"]} - Feature Importance', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.show()

print("\nâœ… Feature Importance:")
for idx, row in importance_df.iterrows():
    print(f"   {row['Feature']:<15} {row['Importance']:.4f}")

# --- Step 7: Create SHAP Explainer ---
print("\nðŸ” Creating explainability model...")
explainer = shap.Explainer(best_model, X_train)

# --- Step 8: Save Models ---
with open('model.pkl', 'wb') as f:
    pickle.dump(best_model, f)
with open('explainer.pkl', 'wb') as f:
    pickle.dump(explainer, f)

print("\nâœ… Models saved successfully!")
print("=" * 70)
print("ðŸŽ‰ Setup complete! Ready to run the web app.")
print("=" * 70)

